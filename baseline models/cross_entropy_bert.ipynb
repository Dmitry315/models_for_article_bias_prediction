{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  dataset='./dataset/polarization_ds.csv'\n",
    "  train_df='./dataset/media_split/train.csv'\n",
    "  valid_df='./dataset/media_split/valid.csv'\n",
    "  test_df='./dataset/media_split/test.csv'\n",
    "  target_cols='bias_text' # 'bias_text', ['left', 'center', 'right'] \n",
    "  classes=3\n",
    "  model='bert-base-cased'\n",
    "  embedd_dim=768\n",
    "  criterion = 'crossentropy' # ['crossentropy', 'mse', 'l1', 'focal']\n",
    "  main_metric = 'f1_macro'\n",
    "  model_file = './models/best_bert_large_ce.pt'\n",
    "  # just use it\n",
    "  apex=True\n",
    "  gradient_checkpointing=True\n",
    "  num_cycles=0.5\n",
    "  num_warmup_steps=0\n",
    "  epochs=5\n",
    "  encoder_lr=2e-5\n",
    "  decoder_lr=2e-5\n",
    "  min_lr=1e-6\n",
    "  eps=1e-6\n",
    "  betas=(0.9, 0.999)\n",
    "  batch_size=32\n",
    "  max_len=512\n",
    "  weight_decay=0.01\n",
    "  # gradient_accumulation_steps=1\n",
    "  max_grad_norm=1\n",
    "  seed=42\n",
    "  scheduler='cosine' # ['linear', 'cosine']\n",
    "  batch_scheduler=True\n",
    "  #\n",
    "  collab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.collab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import os\n",
    "  os.chdir('/content/drive/MyDrive/lab/bert_finetune')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available(): # для GPU отдельный seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(CFG.seed)\n",
    "# есть стохастические операции на GPU\n",
    "# сделаем их детерминированными для воспроизводимости\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG.dataset, index_col='id')\n",
    "train_df = pd.read_csv(CFG.train_df)\n",
    "valid_df = pd.read_csv(CFG.valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>bias_text</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wzYhj24VaSbhhr8T</th>\n",
       "      <td>sexual_misconduct</td>\n",
       "      <td>right</td>\n",
       "      <td>Two more people back parts of Tara Reade’s cla...</td>\n",
       "      <td>Two more people corroborated some of Tara Read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcgmZXUEXb0xWWB1</th>\n",
       "      <td>marijuana_legalization</td>\n",
       "      <td>right</td>\n",
       "      <td>Colorado lawmakers set limits on pot edibles p...</td>\n",
       "      <td>DENVER — The Mile High City was jammed with po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oYxUHzLvAtQhGoXk</th>\n",
       "      <td>foreign_policy</td>\n",
       "      <td>right</td>\n",
       "      <td>Trump, world leaders, mark 100-year WWI annive...</td>\n",
       "      <td>President Donald Trump and dozens of world lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EHAhlgvwQ8U2ZvGB</th>\n",
       "      <td>race_and_racism</td>\n",
       "      <td>right</td>\n",
       "      <td>It’s Not Just MSNBC Making Flip Assumptions Ab...</td>\n",
       "      <td>Last night , the official Twitter feed of MSNB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JD0fSIsxc4p1DuDe</th>\n",
       "      <td>china</td>\n",
       "      <td>center</td>\n",
       "      <td>The Global Stories Of 2019 That You Probably M...</td>\n",
       "      <td>Sure , everybody thinks it 's great when a sto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   topic bias_text   \n",
       "id                                                   \n",
       "wzYhj24VaSbhhr8T       sexual_misconduct     right  \\\n",
       "pcgmZXUEXb0xWWB1  marijuana_legalization     right   \n",
       "oYxUHzLvAtQhGoXk          foreign_policy     right   \n",
       "EHAhlgvwQ8U2ZvGB         race_and_racism     right   \n",
       "JD0fSIsxc4p1DuDe                   china    center   \n",
       "\n",
       "                                                              title   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  Two more people back parts of Tara Reade’s cla...  \\\n",
       "pcgmZXUEXb0xWWB1  Colorado lawmakers set limits on pot edibles p...   \n",
       "oYxUHzLvAtQhGoXk  Trump, world leaders, mark 100-year WWI annive...   \n",
       "EHAhlgvwQ8U2ZvGB  It’s Not Just MSNBC Making Flip Assumptions Ab...   \n",
       "JD0fSIsxc4p1DuDe  The Global Stories Of 2019 That You Probably M...   \n",
       "\n",
       "                                                            content  \n",
       "id                                                                   \n",
       "wzYhj24VaSbhhr8T  Two more people corroborated some of Tara Read...  \n",
       "pcgmZXUEXb0xWWB1  DENVER — The Mile High City was jammed with po...  \n",
       "oYxUHzLvAtQhGoXk  President Donald Trump and dozens of world lea...  \n",
       "EHAhlgvwQ8U2ZvGB  Last night , the official Twitter feed of MSNB...  \n",
       "JD0fSIsxc4p1DuDe  Sure , everybody thinks it 's great when a sto...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = (df['bias_text'] == 'center') + (df['bias_text'] == 'right') * 2\n",
    "df['bias_text'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['left'] = (df['bias_text'] == 0).astype(float)\n",
    "df['center'] = (df['bias_text'] == 1).astype(float)\n",
    "df['right'] = (df['bias_text'] == 2).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[train_df['ID']]\n",
    "valid_df = df.loc[valid_df['ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>bias_text</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>left</th>\n",
       "      <th>center</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zl7kc7EmAyIdUMIo</th>\n",
       "      <td>immigration</td>\n",
       "      <td>2</td>\n",
       "      <td>Shutdown Theater, Again</td>\n",
       "      <td>President Trump and Senate Minority Leader Chu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xpbjYTJYPdlw6HmJ</th>\n",
       "      <td>culture</td>\n",
       "      <td>1</td>\n",
       "      <td>Can the developing world endure the coronavirus?</td>\n",
       "      <td>“ The 360 ” shows you diverse perspectives on ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k4SGI3GXarnz5dJl</th>\n",
       "      <td>elections</td>\n",
       "      <td>0</td>\n",
       "      <td>Sanders’ California supporters can’t quite say...</td>\n",
       "      <td>LOS ANGELES — Actress Rosario Dawson took the ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0jIpietfnrPRGHKQ</th>\n",
       "      <td>white_house</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump says he doesn't know if Rudy Giuliani is...</td>\n",
       "      <td>President Donald Trump said on Friday that he ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zMlSt7dyJvanHqJq</th>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "      <td>Trump's historic moment arrives</td>\n",
       "      <td>Washington ( CNN ) Donald Trump became the 45t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        topic  bias_text   \n",
       "id                                         \n",
       "zl7kc7EmAyIdUMIo  immigration          2  \\\n",
       "xpbjYTJYPdlw6HmJ      culture          1   \n",
       "k4SGI3GXarnz5dJl    elections          0   \n",
       "0jIpietfnrPRGHKQ  white_house          1   \n",
       "zMlSt7dyJvanHqJq     politics          0   \n",
       "\n",
       "                                                              title   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo                            Shutdown Theater, Again  \\\n",
       "xpbjYTJYPdlw6HmJ   Can the developing world endure the coronavirus?   \n",
       "k4SGI3GXarnz5dJl  Sanders’ California supporters can’t quite say...   \n",
       "0jIpietfnrPRGHKQ  Trump says he doesn't know if Rudy Giuliani is...   \n",
       "zMlSt7dyJvanHqJq                    Trump's historic moment arrives   \n",
       "\n",
       "                                                            content  left   \n",
       "id                                                                          \n",
       "zl7kc7EmAyIdUMIo  President Trump and Senate Minority Leader Chu...   0.0  \\\n",
       "xpbjYTJYPdlw6HmJ  “ The 360 ” shows you diverse perspectives on ...   0.0   \n",
       "k4SGI3GXarnz5dJl  LOS ANGELES — Actress Rosario Dawson took the ...   1.0   \n",
       "0jIpietfnrPRGHKQ  President Donald Trump said on Friday that he ...   0.0   \n",
       "zMlSt7dyJvanHqJq  Washington ( CNN ) Donald Trump became the 45t...   1.0   \n",
       "\n",
       "                  center  right  \n",
       "id                               \n",
       "zl7kc7EmAyIdUMIo     0.0    1.0  \n",
       "xpbjYTJYPdlw6HmJ     1.0    0.0  \n",
       "k4SGI3GXarnz5dJl     0.0    0.0  \n",
       "0jIpietfnrPRGHKQ     1.0    0.0  \n",
       "zMlSt7dyJvanHqJq     0.0    0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = (df['title'] + ' ' + df['content']).values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_text(self, idx):\n",
    "        # tokenization\n",
    "        inputs = self.cfg.tokenizer.encode_plus(\n",
    "            self.texts[idx], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def get_labels(self, idx):\n",
    "        #if CFG.criterion != 'crossentropy':\n",
    "        #    return torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return torch.tensor(self.labels[idx]).type(torch.LongTensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.get_text(idx)\n",
    "        label = self.get_labels(idx)\n",
    "        return inputs, label\n",
    "\n",
    "def collate(inputs):\n",
    "\t\t# reduce sequence length\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.model)\n",
    "        if CFG.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.clf =  nn.Linear(CFG.embedd_dim, CFG.classes)\n",
    "        if CFG.criterion != 'crossentropy' and CFG.criterion != 'focal':\n",
    "            self.sm = nn.Softmax(dim=-1)\n",
    "        torch.nn.init.xavier_uniform_(self.clf.weight)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        # sequence has [CLF] token in the beginning\n",
    "        # bert() returns first vector as pooling of sentence\n",
    "        _, x = self.model(input_ids= input_id, attention_mask=mask, return_dict=False)\n",
    "        out = self.clf(x)\n",
    "        if CFG.criterion != 'crossentropy'  and CFG.criterion != 'focal':\n",
    "            return self.sm(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop, metrics and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, pred):\n",
    "  if CFG.criterion != 'crossentropy' and CFG.criterion != 'focal':\n",
    "    y = torch.argmax(y, dim=1)\n",
    "  pred = torch.argmax(pred, dim=1)\n",
    "  f1 = f1_score(y, pred, average='macro')\n",
    "  acc = accuracy_score(y, pred)\n",
    "  return {'f1_macro': f1, 'acc': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.reduction == 'mean': \n",
    "          return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "          return loss.sum()\n",
    "        else:\n",
    "          raise NotImplementedError(f'Not implemented reduction: {self.reduction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TrainDataset(CFG, train_df)\n",
    "valid_ds = TrainDataset(CFG, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.criterion == 'crossentropy':\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "elif CFG.criterion == 'mse':\n",
    "  criterion = nn.MSELoss()\n",
    "elif CFG.criterion == 'l1':\n",
    "  criterion = nn.SmoothL1Loss()\n",
    "elif CFG.criterion == 'focal':\n",
    "  criterion = FocalLoss(5)\n",
    "else:\n",
    "  raise NotImplementedError('Change loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_parameters = get_optimizer_params(model,\n",
    "#                                            encoder_lr=CFG.encoder_lr, \n",
    "#                                            decoder_lr=CFG.decoder_lr,\n",
    "#                                            weight_decay=CFG.weight_decay)\n",
    "#optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=CFG.encoder_lr, \n",
    "                  #eps=CFG.eps, \n",
    "                  betas=CFG.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)\n",
    "scheduler = get_scheduler(CFG, optimizer, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(CFG.device)\n",
    "criterion.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2448289404.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    y_preds = model(input_id, mask)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def train_loop(model, optimizer, criterion, train_loader, valid_loader, epochs, scheduler):\n",
    "  best_score = 0\n",
    "  # multiplies gradient so it won't vanish (torch use float16)\n",
    "  # scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "  for e in range(epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "      inputs = collate(inputs)\n",
    "      # move inputs to device\n",
    "      mask = inputs['attention_mask'].to(CFG.device)\n",
    "      input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "      labels = labels.to(CFG.device)\n",
    "      # forward\n",
    "      # with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "        y_preds = model(input_id, mask)\n",
    "        loss = criterion(y_preds, labels)\n",
    "      # calculate loss\n",
    "      train_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      # scaler.scale(loss).backward()\n",
    "      # gradient clipping\n",
    "      grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "      # scaler.step(optimizer)\n",
    "      # scaler.update()\n",
    "      optimizer.step()\n",
    "      if CFG.batch_scheduler:\n",
    "          scheduler.step()\n",
    "    train_loss = np.mean(train_loss)\n",
    "    # valid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      valid_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for inputs, labels in valid_loader:\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        # with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "        y_preds = model(input_id, mask)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        valid_loss.append(loss.detach().cpu().item())\n",
    "    valid_loss = np.mean(valid_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "    # save best model\n",
    "    if best_score < metrics[CFG.main_metric]:\n",
    "      torch.save(model.state_dict(), CFG.model_file)\n",
    "      best_score = metrics[CFG.main_metric]\n",
    "    print('best_score =', best_score)\n",
    "    print(f'EPOCH {e + 1}:, train_loss = {train_loss}, valid_loss = {valid_loss}', *[f'{name} = {value}' for name, value in metrics.items()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [08:08<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.22713647787894967\n",
      "EPOCH 1:, train_loss = 0.7943838798182106, valid_loss = 2.3569706678390503 f1_macro = 0.22713647787894967 acc = 0.25212224108658743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [08:12<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.22713647787894967\n",
      "EPOCH 2:, train_loss = 0.4399778932225403, valid_loss = 2.8060171153094315 f1_macro = 0.1899071630152068 acc = 0.20500848896434634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [08:11<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.22713647787894967\n",
      "EPOCH 3:, train_loss = 0.33930398156209685, valid_loss = 2.772665658512631 f1_macro = 0.20061660551908314 acc = 0.23132427843803055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [08:11<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.22713647787894967\n",
      "EPOCH 4:, train_loss = 0.28871909121026773, valid_loss = 2.745092105221104 f1_macro = 0.21199500714232125 acc = 0.2597623089983022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [08:12<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.22713647787894967\n",
      "EPOCH 5:, train_loss = 0.2523892844268418, valid_loss = 2.8727190172350086 f1_macro = 0.2070872676739145 acc = 0.24405772495755518\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, optimizer, criterion, train_loader, valid_loader, CFG.epochs, scheduler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, valid_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      valid_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "          y_preds = model(input_id, mask)\n",
    "          loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        valid_loss.append(loss.detach().cpu().item())\n",
    "    valid_loss = np.mean(valid_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "\n",
    "    print(f'Test metrics: valid_loss={valid_loss}', *[f'{name} = {value}' for name, value in metrics.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(CFG.model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(CFG.test_df)\n",
    "test_df = df.loc[test_df['ID']]\n",
    "test_ds = TrainDataset(CFG, test_df)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: valid_loss=2.3569706678390503 f1_macro = 0.22713647787894967 acc = 0.25212224108658743\n"
     ]
    }
   ],
   "source": [
    "test(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: valid_loss=1.4832368914673968 f1_macro = 0.38279101560086454 acc = 0.39076923076923076\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
