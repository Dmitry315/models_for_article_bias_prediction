{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  dataset='./dataset/polarization_ds.csv'\n",
    "  train_df='./dataset/media_split/train.csv'\n",
    "  valid_df='./dataset/media_split/valid.csv'\n",
    "  test_df='./dataset/media_split/test.csv'\n",
    "  target_cols=['left', 'center', 'right'] # 'bias_text'\n",
    "  classes=3\n",
    "  model='microsoft/deberta-v3-base'\n",
    "  embedd_dim=768\n",
    "  criterion = 'mse' # ['crossentropy', 'mse', 'l1', 'focal']\n",
    "  main_metric = 'f1_macro'\n",
    "  model_file = './models/best_deberta_base.pt'\n",
    "  # just use it\n",
    "  apex=True\n",
    "  gradient_checkpointing=True\n",
    "  num_cycles=0.5\n",
    "  num_warmup_steps=0\n",
    "  epochs=5\n",
    "  encoder_lr=2e-5\n",
    "  decoder_lr=2e-5\n",
    "  min_lr=1e-6\n",
    "  eps=1e-6\n",
    "  betas=(0.9, 0.999)\n",
    "  batch_size=32\n",
    "  max_len=512\n",
    "  weight_decay=0.01\n",
    "  # gradient_accumulation_steps=1\n",
    "  max_grad_norm=1\n",
    "  seed=42\n",
    "  scheduler='cosine' # ['linear', 'cosine']\n",
    "  batch_scheduler=True\n",
    "  #\n",
    "  colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.colab:\n",
    "  from google.collab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import os\n",
    "  os.chdir('/content/drive/MyDrive/lab/bert_finetune')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tokenizers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available(): # для GPU отдельный seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(CFG.seed)\n",
    "# есть стохастические операции на GPU\n",
    "# сделаем их детерминированными для воспроизводимости\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG.dataset, index_col='id')\n",
    "train_df = pd.read_csv(CFG.train_df)\n",
    "valid_df = pd.read_csv(CFG.valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>bias_text</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wzYhj24VaSbhhr8T</th>\n",
       "      <td>sexual_misconduct</td>\n",
       "      <td>right</td>\n",
       "      <td>Two more people back parts of Tara Reade’s cla...</td>\n",
       "      <td>Two more people corroborated some of Tara Read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcgmZXUEXb0xWWB1</th>\n",
       "      <td>marijuana_legalization</td>\n",
       "      <td>right</td>\n",
       "      <td>Colorado lawmakers set limits on pot edibles p...</td>\n",
       "      <td>DENVER — The Mile High City was jammed with po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oYxUHzLvAtQhGoXk</th>\n",
       "      <td>foreign_policy</td>\n",
       "      <td>right</td>\n",
       "      <td>Trump, world leaders, mark 100-year WWI annive...</td>\n",
       "      <td>President Donald Trump and dozens of world lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EHAhlgvwQ8U2ZvGB</th>\n",
       "      <td>race_and_racism</td>\n",
       "      <td>right</td>\n",
       "      <td>It’s Not Just MSNBC Making Flip Assumptions Ab...</td>\n",
       "      <td>Last night , the official Twitter feed of MSNB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JD0fSIsxc4p1DuDe</th>\n",
       "      <td>china</td>\n",
       "      <td>center</td>\n",
       "      <td>The Global Stories Of 2019 That You Probably M...</td>\n",
       "      <td>Sure , everybody thinks it 's great when a sto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   topic bias_text   \n",
       "id                                                   \n",
       "wzYhj24VaSbhhr8T       sexual_misconduct     right  \\\n",
       "pcgmZXUEXb0xWWB1  marijuana_legalization     right   \n",
       "oYxUHzLvAtQhGoXk          foreign_policy     right   \n",
       "EHAhlgvwQ8U2ZvGB         race_and_racism     right   \n",
       "JD0fSIsxc4p1DuDe                   china    center   \n",
       "\n",
       "                                                              title   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  Two more people back parts of Tara Reade’s cla...  \\\n",
       "pcgmZXUEXb0xWWB1  Colorado lawmakers set limits on pot edibles p...   \n",
       "oYxUHzLvAtQhGoXk  Trump, world leaders, mark 100-year WWI annive...   \n",
       "EHAhlgvwQ8U2ZvGB  It’s Not Just MSNBC Making Flip Assumptions Ab...   \n",
       "JD0fSIsxc4p1DuDe  The Global Stories Of 2019 That You Probably M...   \n",
       "\n",
       "                                                            content  \n",
       "id                                                                   \n",
       "wzYhj24VaSbhhr8T  Two more people corroborated some of Tara Read...  \n",
       "pcgmZXUEXb0xWWB1  DENVER — The Mile High City was jammed with po...  \n",
       "oYxUHzLvAtQhGoXk  President Donald Trump and dozens of world lea...  \n",
       "EHAhlgvwQ8U2ZvGB  Last night , the official Twitter feed of MSNB...  \n",
       "JD0fSIsxc4p1DuDe  Sure , everybody thinks it 's great when a sto...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = (df['bias_text'] == 'center') + (df['bias_text'] == 'right') * 2\n",
    "df['bias_text'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['left'] = (df['bias_text'] == 0).astype(float)\n",
    "df['center'] = (df['bias_text'] == 1).astype(float)\n",
    "df['right'] = (df['bias_text'] == 2).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[train_df['ID']]\n",
    "valid_df = df.loc[valid_df['ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>bias_text</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>left</th>\n",
       "      <th>center</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zl7kc7EmAyIdUMIo</th>\n",
       "      <td>immigration</td>\n",
       "      <td>2</td>\n",
       "      <td>Shutdown Theater, Again</td>\n",
       "      <td>President Trump and Senate Minority Leader Chu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xpbjYTJYPdlw6HmJ</th>\n",
       "      <td>culture</td>\n",
       "      <td>1</td>\n",
       "      <td>Can the developing world endure the coronavirus?</td>\n",
       "      <td>“ The 360 ” shows you diverse perspectives on ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k4SGI3GXarnz5dJl</th>\n",
       "      <td>elections</td>\n",
       "      <td>0</td>\n",
       "      <td>Sanders’ California supporters can’t quite say...</td>\n",
       "      <td>LOS ANGELES — Actress Rosario Dawson took the ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0jIpietfnrPRGHKQ</th>\n",
       "      <td>white_house</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump says he doesn't know if Rudy Giuliani is...</td>\n",
       "      <td>President Donald Trump said on Friday that he ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zMlSt7dyJvanHqJq</th>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "      <td>Trump's historic moment arrives</td>\n",
       "      <td>Washington ( CNN ) Donald Trump became the 45t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        topic  bias_text   \n",
       "id                                         \n",
       "zl7kc7EmAyIdUMIo  immigration          2  \\\n",
       "xpbjYTJYPdlw6HmJ      culture          1   \n",
       "k4SGI3GXarnz5dJl    elections          0   \n",
       "0jIpietfnrPRGHKQ  white_house          1   \n",
       "zMlSt7dyJvanHqJq     politics          0   \n",
       "\n",
       "                                                              title   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo                            Shutdown Theater, Again  \\\n",
       "xpbjYTJYPdlw6HmJ   Can the developing world endure the coronavirus?   \n",
       "k4SGI3GXarnz5dJl  Sanders’ California supporters can’t quite say...   \n",
       "0jIpietfnrPRGHKQ  Trump says he doesn't know if Rudy Giuliani is...   \n",
       "zMlSt7dyJvanHqJq                    Trump's historic moment arrives   \n",
       "\n",
       "                                                            content  left   \n",
       "id                                                                          \n",
       "zl7kc7EmAyIdUMIo  President Trump and Senate Minority Leader Chu...   0.0  \\\n",
       "xpbjYTJYPdlw6HmJ  “ The 360 ” shows you diverse perspectives on ...   0.0   \n",
       "k4SGI3GXarnz5dJl  LOS ANGELES — Actress Rosario Dawson took the ...   1.0   \n",
       "0jIpietfnrPRGHKQ  President Donald Trump said on Friday that he ...   0.0   \n",
       "zMlSt7dyJvanHqJq  Washington ( CNN ) Donald Trump became the 45t...   1.0   \n",
       "\n",
       "                  center  right  \n",
       "id                               \n",
       "zl7kc7EmAyIdUMIo     0.0    1.0  \n",
       "xpbjYTJYPdlw6HmJ     1.0    0.0  \n",
       "k4SGI3GXarnz5dJl     0.0    0.0  \n",
       "0jIpietfnrPRGHKQ     1.0    0.0  \n",
       "zMlSt7dyJvanHqJq     0.0    0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = (df['title'] + ' ' + df['content']).values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_text(self, idx):\n",
    "        # tokenization\n",
    "        inputs = self.cfg.tokenizer.encode_plus(\n",
    "            self.texts[idx], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            # padding='longest',\n",
    "            truncation=True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "    \n",
    "    def get_labels(self, idx):\n",
    "        if CFG.criterion != 'crossentropy' and CFG.criterion != 'focal':\n",
    "           return torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return torch.tensor(self.labels[idx]).type(torch.LongTensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.get_text(idx)\n",
    "        label = self.get_labels(idx)\n",
    "        return inputs, label\n",
    "\n",
    "def collate(inputs):\n",
    "\t\t# reduce sequence length\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.model)\n",
    "        if CFG.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.clf =  nn.Linear(CFG.embedd_dim, CFG.classes)\n",
    "        if not CFG.criterion == 'crossentropy':\n",
    "            self.sm = nn.Softmax(dim=-1)\n",
    "        torch.nn.init.xavier_uniform_(self.clf.weight)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        # sequence has [CLF] token in the beginning\n",
    "        # bert() returns first vector as pooling of sentence\n",
    "        x = self.model(input_ids= input_id, attention_mask=mask)[0]\n",
    "        out = self.pool(x, mask)\n",
    "        out = self.clf(out)\n",
    "        if not CFG.criterion == 'crossentropy':\n",
    "            return self.sm(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop, metrics and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, pred):\n",
    "  y = torch.argmax(y, dim=1)\n",
    "  pred = torch.argmax(pred, dim=1)\n",
    "  f1 = f1_score(y, pred, average='macro')\n",
    "  acc = accuracy_score(y, pred)\n",
    "  return {'f1_macro': f1, 'acc': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.reduction == 'mean': \n",
    "          return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "          return loss.sum()\n",
    "        else:\n",
    "          raise NotImplementedError(f'Not implemented reduction: {self.reduction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TrainDataset(CFG, train_df)\n",
    "valid_ds = TrainDataset(CFG, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.criterion == 'crossentropy':\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "elif CFG.criterion == 'mse':\n",
    "  criterion = nn.MSELoss()\n",
    "elif CFG.criterion == 'l1':\n",
    "  criterion = nn.SmoothL1Loss()\n",
    "elif CFG.criterion == 'focal':\n",
    "  criterion = FocalLoss(5)\n",
    "else:\n",
    "  raise NotImplementedError('Change loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_parameters = get_optimizer_params(model,\n",
    "                                           encoder_lr=CFG.encoder_lr, \n",
    "                                           decoder_lr=CFG.decoder_lr,\n",
    "                                           weight_decay=CFG.weight_decay)\n",
    "optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)\n",
    "scheduler = get_scheduler(CFG, optimizer, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(CFG.device)\n",
    "criterion.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, train_loader, valid_loader, epochs, scheduler):\n",
    "  best_score = 0\n",
    "  # multiplies gradient so it won't vanish (torch use float16)\n",
    "  scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "  for e in range(epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "      inputs = collate(inputs)\n",
    "      # move inputs to device\n",
    "      mask = inputs['attention_mask'].to(CFG.device)\n",
    "      input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "      labels = labels.to(CFG.device)\n",
    "      # forward\n",
    "      with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "        y_preds = model(input_id, mask)\n",
    "        loss = criterion(y_preds, labels)\n",
    "      # calculate loss\n",
    "      train_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      # loss.backward()\n",
    "      scaler.scale(loss).backward()\n",
    "      # gradient clipping\n",
    "      grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "      # optimizer.step()\n",
    "      if CFG.batch_scheduler:\n",
    "          scheduler.step()\n",
    "    train_loss = np.mean(train_loss)\n",
    "    # valid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      valid_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for inputs, labels in valid_loader:\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "          y_preds = model(input_id, mask)\n",
    "          loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        valid_loss.append(loss.detach().cpu().item())\n",
    "    valid_loss = np.mean(valid_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "    # save best model\n",
    "    if best_score < metrics[CFG.main_metric]:\n",
    "      torch.save(model.state_dict(), CFG.model_file)\n",
    "      best_score = metrics[CFG.main_metric]\n",
    "    print('best_score =', best_score)\n",
    "    print(f'EPOCH {e + 1}:, train_loss = {train_loss}, valid_loss = {valid_loss}', *[f'{name} = {value}' for name, value in metrics.items()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "100%|██████████| 416/416 [14:02<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.275029626415907\n",
      "EPOCH 1:, train_loss = 0.24768156379174727, valid_loss = 0.2216248935138857 f1_macro = 0.275029626415907 acc = 0.4367572156196944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 416/416 [14:04<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.3058787095987227\n",
      "EPOCH 2:, train_loss = 0.21367977294497764, valid_loss = 0.21490701992769498 f1_macro = 0.3058787095987227 acc = 0.4859932088285229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 416/416 [14:03<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.3458828365790134\n",
      "EPOCH 3:, train_loss = 0.20428220025048807, valid_loss = 0.20512174714255976 f1_macro = 0.3458828365790134 acc = 0.5411714770797963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 416/416 [14:06<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.36821932841059374\n",
      "EPOCH 4:, train_loss = 0.19305806026722377, valid_loss = 0.20510840697868452 f1_macro = 0.36821932841059374 acc = 0.5246179966044142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 416/416 [14:05<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.36821932841059374\n",
      "EPOCH 5:, train_loss = 0.18241622069707283, valid_loss = 0.20925698046748703 f1_macro = 0.3464206238941396 acc = 0.47283531409168084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 416/416 [14:04<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.36821932841059374\n",
      "EPOCH 6:, train_loss = 0.1770691338998194, valid_loss = 0.20700339852152644 f1_macro = 0.3488355454069236 acc = 0.4847198641765705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 416/416 [14:04<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.36821932841059374\n",
      "EPOCH 7:, train_loss = 0.17467433522240475, valid_loss = 0.20937290586329796 f1_macro = 0.3361847371919315 acc = 0.4592529711375212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 416/416 [14:05<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.36821932841059374\n",
      "EPOCH 8:, train_loss = 0.17358122286028588, valid_loss = 0.20937477897953344 f1_macro = 0.3363956204376075 acc = 0.4588285229202037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 416/416 [14:05<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.36821932841059374\n",
      "EPOCH 9:, train_loss = 0.17331649187522438, valid_loss = 0.2100283108853005 f1_macro = 0.3345209280511991 acc = 0.45543293718166383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/416 [00:00<?, ?it/s]/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 416/416 [14:05<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.36821932841059374\n",
      "EPOCH 10:, train_loss = 0.1729615069925785, valid_loss = 0.20996148441288923 f1_macro = 0.33512637210455276 acc = 0.45585738539898135\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, optimizer, criterion, train_loader, valid_loader, CFG.epochs, scheduler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      test_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for step, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "          y_preds = model(input_id, mask)\n",
    "          loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        test_loss.append(loss.detach().cpu().item())\n",
    "    test_loss = np.mean(test_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "\n",
    "    print(f'Test metrics: test_loss={test_loss}', *[f'{name} = {value}' for name, value in metrics.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(CFG.model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(CFG.test_df)\n",
    "test_df = df.loc[test_df['ID']]\n",
    "test_ds = TrainDataset(CFG, test_df)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss=0.20510840697868452 f1_macro = 0.36821932841059374 acc = 0.5246179966044142\n"
     ]
    }
   ],
   "source": [
    "test(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.pyenv/versions/3.10.10/envs/test_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss=0.20383579177515848 f1_macro = 0.3883497747222216 acc = 0.4946153846153846\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
