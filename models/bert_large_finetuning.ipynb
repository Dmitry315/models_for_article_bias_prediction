{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  dataset='./dataset/polarization_ds.csv'\n",
    "  train_df='./dataset/media_split/train.csv'\n",
    "  valid_df='./dataset/media_split/valid.csv'\n",
    "  test_df='./dataset/media_split/test.csv'\n",
    "  target_cols=['left', 'center', 'right'] # 'bias_text'\n",
    "  classes=3\n",
    "  model='bert-large-cased'\n",
    "  embedd_dim=1024\n",
    "  criterion = 'mse' # ['crossentropy', 'mse', 'l1', 'focal']\n",
    "  main_metric = 'f1_macro'\n",
    "  model_file = './models/best_bert_large.pt'\n",
    "  triplet_pretrain = False\n",
    "  emotion_pretrain = True\n",
    "  triplet_model = './models/bert_large_triplet.pt'\n",
    "  emotion_model = '../emotion/detection/models/ed_best_bert_large.pt'\n",
    "  # just use it\n",
    "  apex=True\n",
    "  gradient_checkpointing=True\n",
    "  num_cycles=0.5\n",
    "  num_warmup_steps=0\n",
    "  epochs=5\n",
    "  encoder_lr=2e-5\n",
    "  decoder_lr=2e-5\n",
    "  min_lr=1e-6\n",
    "  eps=1e-6\n",
    "  betas=(0.9, 0.999)\n",
    "  batch_size=32\n",
    "  max_len=512\n",
    "  weight_decay=0.01\n",
    "  # gradient_accumulation_steps=1\n",
    "  max_grad_norm=1\n",
    "  seed=0\n",
    "  scheduler='cosine' # ['linear', 'cosine']\n",
    "  batch_scheduler=True\n",
    "  #\n",
    "  colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import os\n",
    "  os.chdir('/content/drive/MyDrive/lab/bert_finetune')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available(): # для GPU отдельный seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(CFG.seed)\n",
    "# есть стохастические операции на GPU\n",
    "# сделаем их детерминированными для воспроизводимости\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG.dataset, index_col='id')\n",
    "train_df = pd.read_csv(CFG.train_df)\n",
    "valid_df = pd.read_csv(CFG.valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>source</th>\n",
       "      <th>bias</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>authors</th>\n",
       "      <th>content</th>\n",
       "      <th>content_original</th>\n",
       "      <th>source_url</th>\n",
       "      <th>bias_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wzYhj24VaSbhhr8T</th>\n",
       "      <td>sexual_misconduct</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://nypost.com/2020/05/03/two-more-people-...</td>\n",
       "      <td>Two more people back parts of Tara Reade’s cla...</td>\n",
       "      <td>2020-05-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two more people corroborated some of Tara Read...</td>\n",
       "      <td>Two more people corroborated some of Tara Read...</td>\n",
       "      <td>www.nypost.com</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcgmZXUEXb0xWWB1</th>\n",
       "      <td>marijuana_legalization</td>\n",
       "      <td>Washington Times</td>\n",
       "      <td>2.0</td>\n",
       "      <td>http://www.washingtontimes.com/news/2014/apr/2...</td>\n",
       "      <td>Colorado lawmakers set limits on pot edibles p...</td>\n",
       "      <td>2014-04-21</td>\n",
       "      <td>Valerie Richardson</td>\n",
       "      <td>DENVER — The Mile High City was jammed with po...</td>\n",
       "      <td>DENVER — The Mile High City was jammed with po...</td>\n",
       "      <td>www.washingtontimes.com</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oYxUHzLvAtQhGoXk</th>\n",
       "      <td>foreign_policy</td>\n",
       "      <td>TheBlaze.com</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.theblaze.com/news/2018/11/11/trump...</td>\n",
       "      <td>Trump, world leaders, mark 100-year WWI annive...</td>\n",
       "      <td>2018-11-11</td>\n",
       "      <td>Teri Webster</td>\n",
       "      <td>President Donald Trump and dozens of world lea...</td>\n",
       "      <td>President Donald Trump and dozens of world lea...</td>\n",
       "      <td>www.theblaze.com</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EHAhlgvwQ8U2ZvGB</th>\n",
       "      <td>race_and_racism</td>\n",
       "      <td>Matt Welch</td>\n",
       "      <td>2.0</td>\n",
       "      <td>http://reason.com/blog/2014/01/30/its-not-just...</td>\n",
       "      <td>It’s Not Just MSNBC Making Flip Assumptions Ab...</td>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>Josh Blackman, Scott Shackford, Shikha Dalmia,...</td>\n",
       "      <td>Last night , the official Twitter feed of MSNB...</td>\n",
       "      <td>Last night, the official Twitter feed of MSNBC...</td>\n",
       "      <td>www.reason.com</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JD0fSIsxc4p1DuDe</th>\n",
       "      <td>china</td>\n",
       "      <td>NPR Online News</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.npr.org/sections/goatsandsoda/2019...</td>\n",
       "      <td>The Global Stories Of 2019 That You Probably M...</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>Marc Silver</td>\n",
       "      <td>Sure , everybody thinks it 's great when a sto...</td>\n",
       "      <td>The Global Stories Of 2019 That You Probably M...</td>\n",
       "      <td>www.npr.org</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   topic            source  bias   \n",
       "id                                                                 \n",
       "wzYhj24VaSbhhr8T       sexual_misconduct     New York Post   2.0  \\\n",
       "pcgmZXUEXb0xWWB1  marijuana_legalization  Washington Times   2.0   \n",
       "oYxUHzLvAtQhGoXk          foreign_policy      TheBlaze.com   2.0   \n",
       "EHAhlgvwQ8U2ZvGB         race_and_racism        Matt Welch   2.0   \n",
       "JD0fSIsxc4p1DuDe                   china   NPR Online News   1.0   \n",
       "\n",
       "                                                                url   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  https://nypost.com/2020/05/03/two-more-people-...  \\\n",
       "pcgmZXUEXb0xWWB1  http://www.washingtontimes.com/news/2014/apr/2...   \n",
       "oYxUHzLvAtQhGoXk  https://www.theblaze.com/news/2018/11/11/trump...   \n",
       "EHAhlgvwQ8U2ZvGB  http://reason.com/blog/2014/01/30/its-not-just...   \n",
       "JD0fSIsxc4p1DuDe  https://www.npr.org/sections/goatsandsoda/2019...   \n",
       "\n",
       "                                                              title   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  Two more people back parts of Tara Reade’s cla...  \\\n",
       "pcgmZXUEXb0xWWB1  Colorado lawmakers set limits on pot edibles p...   \n",
       "oYxUHzLvAtQhGoXk  Trump, world leaders, mark 100-year WWI annive...   \n",
       "EHAhlgvwQ8U2ZvGB  It’s Not Just MSNBC Making Flip Assumptions Ab...   \n",
       "JD0fSIsxc4p1DuDe  The Global Stories Of 2019 That You Probably M...   \n",
       "\n",
       "                        date   \n",
       "id                             \n",
       "wzYhj24VaSbhhr8T  2020-05-03  \\\n",
       "pcgmZXUEXb0xWWB1  2014-04-21   \n",
       "oYxUHzLvAtQhGoXk  2018-11-11   \n",
       "EHAhlgvwQ8U2ZvGB  2014-01-30   \n",
       "JD0fSIsxc4p1DuDe  2019-12-28   \n",
       "\n",
       "                                                            authors   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T                                                NaN  \\\n",
       "pcgmZXUEXb0xWWB1                                 Valerie Richardson   \n",
       "oYxUHzLvAtQhGoXk                                       Teri Webster   \n",
       "EHAhlgvwQ8U2ZvGB  Josh Blackman, Scott Shackford, Shikha Dalmia,...   \n",
       "JD0fSIsxc4p1DuDe                                        Marc Silver   \n",
       "\n",
       "                                                            content   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  Two more people corroborated some of Tara Read...  \\\n",
       "pcgmZXUEXb0xWWB1  DENVER — The Mile High City was jammed with po...   \n",
       "oYxUHzLvAtQhGoXk  President Donald Trump and dozens of world lea...   \n",
       "EHAhlgvwQ8U2ZvGB  Last night , the official Twitter feed of MSNB...   \n",
       "JD0fSIsxc4p1DuDe  Sure , everybody thinks it 's great when a sto...   \n",
       "\n",
       "                                                   content_original   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  Two more people corroborated some of Tara Read...  \\\n",
       "pcgmZXUEXb0xWWB1  DENVER — The Mile High City was jammed with po...   \n",
       "oYxUHzLvAtQhGoXk  President Donald Trump and dozens of world lea...   \n",
       "EHAhlgvwQ8U2ZvGB  Last night, the official Twitter feed of MSNBC...   \n",
       "JD0fSIsxc4p1DuDe  The Global Stories Of 2019 That You Probably M...   \n",
       "\n",
       "                               source_url bias_text  \n",
       "id                                                   \n",
       "wzYhj24VaSbhhr8T           www.nypost.com     right  \n",
       "pcgmZXUEXb0xWWB1  www.washingtontimes.com     right  \n",
       "oYxUHzLvAtQhGoXk         www.theblaze.com     right  \n",
       "EHAhlgvwQ8U2ZvGB           www.reason.com     right  \n",
       "JD0fSIsxc4p1DuDe              www.npr.org    center  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = (df['bias_text'] == 'center') + (df['bias_text'] == 'right') * 2\n",
    "df['bias_text'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['left'] = (df['bias_text'] == 0).astype(float)\n",
    "df['center'] = (df['bias_text'] == 1).astype(float)\n",
    "df['right'] = (df['bias_text'] == 2).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[train_df['ID']]\n",
    "valid_df = df.loc[valid_df['ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>source</th>\n",
       "      <th>bias</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>authors</th>\n",
       "      <th>content</th>\n",
       "      <th>content_original</th>\n",
       "      <th>source_url</th>\n",
       "      <th>bias_text</th>\n",
       "      <th>left</th>\n",
       "      <th>center</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zl7kc7EmAyIdUMIo</th>\n",
       "      <td>immigration</td>\n",
       "      <td>National Review</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.nationalreview.com/2018/12/governm...</td>\n",
       "      <td>Shutdown Theater, Again</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>Kevin D. Williamson, Kyle Smith, Andrew C. Mcc...</td>\n",
       "      <td>President Trump and Senate Minority Leader Chu...</td>\n",
       "      <td>President Trump and Senate Minority Leader Chu...</td>\n",
       "      <td>www.nationalreview.com</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xpbjYTJYPdlw6HmJ</th>\n",
       "      <td>culture</td>\n",
       "      <td>Yahoo! The 360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://news.yahoo.com/can-the-developing-worl...</td>\n",
       "      <td>Can the developing world endure the coronavirus?</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>Mike Bebernes</td>\n",
       "      <td>“ The 360 ” shows you diverse perspectives on ...</td>\n",
       "      <td>“The 360” shows you diverse perspectives on th...</td>\n",
       "      <td>www.news.yahoo.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k4SGI3GXarnz5dJl</th>\n",
       "      <td>elections</td>\n",
       "      <td>Politico</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://www.politico.com/story/2016/07/bernie-s...</td>\n",
       "      <td>Sanders’ California supporters can’t quite say...</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>Daniel Strauss, Henry C. Jackson, Nick Gass</td>\n",
       "      <td>LOS ANGELES — Actress Rosario Dawson took the ...</td>\n",
       "      <td>LOS ANGELES — Actress Rosario Dawson took the ...</td>\n",
       "      <td>www.politico.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0jIpietfnrPRGHKQ</th>\n",
       "      <td>white_house</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.businessinsider.com/trump-distance...</td>\n",
       "      <td>Trump says he doesn't know if Rudy Giuliani is...</td>\n",
       "      <td>2019-10-11</td>\n",
       "      <td>Sonam Sheth</td>\n",
       "      <td>President Donald Trump said on Friday that he ...</td>\n",
       "      <td>President Donald Trump said on Friday that he ...</td>\n",
       "      <td>www.businessinsider.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zMlSt7dyJvanHqJq</th>\n",
       "      <td>politics</td>\n",
       "      <td>CNN (Web News)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://www.cnn.com/2017/01/20/politics/donald-...</td>\n",
       "      <td>Trump's historic moment arrives</td>\n",
       "      <td>2017-01-20</td>\n",
       "      <td>Stephen Collinson</td>\n",
       "      <td>Washington ( CNN ) Donald Trump became the 45t...</td>\n",
       "      <td>Washington (CNN) Donald Trump became the 45th ...</td>\n",
       "      <td>www.cnn.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        topic            source  bias   \n",
       "id                                                      \n",
       "zl7kc7EmAyIdUMIo  immigration   National Review   2.0  \\\n",
       "xpbjYTJYPdlw6HmJ      culture    Yahoo! The 360   1.0   \n",
       "k4SGI3GXarnz5dJl    elections          Politico   0.0   \n",
       "0jIpietfnrPRGHKQ  white_house  Business Insider   1.0   \n",
       "zMlSt7dyJvanHqJq     politics    CNN (Web News)   0.0   \n",
       "\n",
       "                                                                url   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo  https://www.nationalreview.com/2018/12/governm...  \\\n",
       "xpbjYTJYPdlw6HmJ  https://news.yahoo.com/can-the-developing-worl...   \n",
       "k4SGI3GXarnz5dJl  http://www.politico.com/story/2016/07/bernie-s...   \n",
       "0jIpietfnrPRGHKQ  https://www.businessinsider.com/trump-distance...   \n",
       "zMlSt7dyJvanHqJq  http://www.cnn.com/2017/01/20/politics/donald-...   \n",
       "\n",
       "                                                              title   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo                            Shutdown Theater, Again  \\\n",
       "xpbjYTJYPdlw6HmJ   Can the developing world endure the coronavirus?   \n",
       "k4SGI3GXarnz5dJl  Sanders’ California supporters can’t quite say...   \n",
       "0jIpietfnrPRGHKQ  Trump says he doesn't know if Rudy Giuliani is...   \n",
       "zMlSt7dyJvanHqJq                    Trump's historic moment arrives   \n",
       "\n",
       "                        date   \n",
       "id                             \n",
       "zl7kc7EmAyIdUMIo  2018-12-12  \\\n",
       "xpbjYTJYPdlw6HmJ  2020-06-30   \n",
       "k4SGI3GXarnz5dJl  2016-07-02   \n",
       "0jIpietfnrPRGHKQ  2019-10-11   \n",
       "zMlSt7dyJvanHqJq  2017-01-20   \n",
       "\n",
       "                                                            authors   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo  Kevin D. Williamson, Kyle Smith, Andrew C. Mcc...  \\\n",
       "xpbjYTJYPdlw6HmJ                                      Mike Bebernes   \n",
       "k4SGI3GXarnz5dJl        Daniel Strauss, Henry C. Jackson, Nick Gass   \n",
       "0jIpietfnrPRGHKQ                                        Sonam Sheth   \n",
       "zMlSt7dyJvanHqJq                                  Stephen Collinson   \n",
       "\n",
       "                                                            content   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo  President Trump and Senate Minority Leader Chu...  \\\n",
       "xpbjYTJYPdlw6HmJ  “ The 360 ” shows you diverse perspectives on ...   \n",
       "k4SGI3GXarnz5dJl  LOS ANGELES — Actress Rosario Dawson took the ...   \n",
       "0jIpietfnrPRGHKQ  President Donald Trump said on Friday that he ...   \n",
       "zMlSt7dyJvanHqJq  Washington ( CNN ) Donald Trump became the 45t...   \n",
       "\n",
       "                                                   content_original   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo  President Trump and Senate Minority Leader Chu...  \\\n",
       "xpbjYTJYPdlw6HmJ  “The 360” shows you diverse perspectives on th...   \n",
       "k4SGI3GXarnz5dJl  LOS ANGELES — Actress Rosario Dawson took the ...   \n",
       "0jIpietfnrPRGHKQ  President Donald Trump said on Friday that he ...   \n",
       "zMlSt7dyJvanHqJq  Washington (CNN) Donald Trump became the 45th ...   \n",
       "\n",
       "                               source_url  bias_text  left  center  right  \n",
       "id                                                                         \n",
       "zl7kc7EmAyIdUMIo   www.nationalreview.com          2   0.0     0.0    1.0  \n",
       "xpbjYTJYPdlw6HmJ       www.news.yahoo.com          1   0.0     1.0    0.0  \n",
       "k4SGI3GXarnz5dJl         www.politico.com          0   1.0     0.0    0.0  \n",
       "0jIpietfnrPRGHKQ  www.businessinsider.com          1   0.0     1.0    0.0  \n",
       "zMlSt7dyJvanHqJq              www.cnn.com          0   1.0     0.0    0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = (df['title'] + ' ' + df['content']).values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_text(self, idx):\n",
    "        # tokenization\n",
    "        inputs = self.cfg.tokenizer.encode_plus(\n",
    "            self.texts[idx], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "    \n",
    "    def get_labels(self, idx):\n",
    "        if CFG.criterion != 'crossentropy' and CFG.criterion != 'focal':\n",
    "           return torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return torch.tensor(self.labels[idx]).type(torch.LongTensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.get_text(idx)\n",
    "        label = self.get_labels(idx)\n",
    "        return inputs, label\n",
    "\n",
    "def collate(inputs):\n",
    "\t\t# reduce sequence length\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.model)\n",
    "        if CFG.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.clf =  nn.Linear(CFG.embedd_dim, CFG.classes)\n",
    "        if not CFG.criterion == 'crossentropy':\n",
    "            self.sm = nn.Softmax(dim=-1)\n",
    "        torch.nn.init.xavier_uniform_(self.clf.weight)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        # sequence has [CLF] token in the beginning\n",
    "        # bert() returns first vector as pooling of sentence\n",
    "        _, x = self.model(input_ids= input_id, attention_mask=mask, return_dict=False)\n",
    "        out = self.clf(x)\n",
    "        if not CFG.criterion == 'crossentropy':\n",
    "            return self.sm(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop, metrics and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, pred):\n",
    "  y = torch.argmax(y, dim=1)\n",
    "  pred = torch.argmax(pred, dim=1)\n",
    "  f1 = f1_score(y, pred, average='macro')\n",
    "  acc = accuracy_score(y, pred)\n",
    "  return {'f1_macro': f1, 'acc': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.reduction == 'mean': \n",
    "          return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "          return loss.sum()\n",
    "        else:\n",
    "          raise NotImplementedError(f'Not implemented reduction: {self.reduction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TrainDataset(CFG, train_df)\n",
    "valid_ds = TrainDataset(CFG, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.criterion == 'crossentropy':\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "elif CFG.criterion == 'mse':\n",
    "  criterion = nn.MSELoss()\n",
    "elif CFG.criterion == 'l1':\n",
    "  criterion = nn.SmoothL1Loss()\n",
    "elif CFG.criterion == 'focal':\n",
    "  criterion = FocalLoss(5)\n",
    "else:\n",
    "  raise NotImplementedError('Change loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_parameters = get_optimizer_params(model,\n",
    "#                                            encoder_lr=CFG.encoder_lr, \n",
    "#                                            decoder_lr=CFG.decoder_lr,\n",
    "#                                            weight_decay=CFG.weight_decay)\n",
    "#optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)\n",
    "scheduler = get_scheduler(CFG, optimizer, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(CFG.device)\n",
    "criterion.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, train_loader, valid_loader, epochs, scheduler):\n",
    "  best_score = 0\n",
    "  # multiplies gradient so it won't vanish (torch use float16)\n",
    "  scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "  for e in range(epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "      inputs = collate(inputs)\n",
    "      # move inputs to device\n",
    "      mask = inputs['attention_mask'].to(CFG.device)\n",
    "      input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "      labels = labels.to(CFG.device)\n",
    "      # forward\n",
    "      with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "        y_preds = model(input_id, mask)\n",
    "        loss = criterion(y_preds, labels)\n",
    "      # calculate loss\n",
    "      train_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      # loss.backward()\n",
    "      scaler.scale(loss).backward()\n",
    "      # gradient clipping\n",
    "      grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "      # optimizer.step()\n",
    "      if CFG.batch_scheduler:\n",
    "          scheduler.step()\n",
    "    train_loss = np.mean(train_loss)\n",
    "    # valid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      valid_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for inputs, labels in valid_loader:\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        # with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "        y_preds = model(input_id, mask)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        valid_loss.append(loss.detach().cpu().item())\n",
    "    valid_loss = np.mean(valid_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "    # save best model\n",
    "    if best_score < metrics[CFG.main_metric]:\n",
    "      torch.save(model.state_dict(), CFG.model_file) \n",
    "      best_score = metrics[CFG.main_metric]\n",
    "    print('best_score =', best_score)\n",
    "    print(f'EPOCH {e + 1}:, train_loss = {train_loss}, valid_loss = {valid_loss}', *[f'{name} = {value}' for name, value in metrics.items()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.triplet_pretrain:\n",
    "    model.load_state_dict(torch.load(CFG.triplet_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [20:41<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.02662320021733225\n",
      "EPOCH 1:, train_loss = 0.2666582520401148, valid_loss = 0.3199017362014667 f1_macro = 0.02662320021733225 acc = 0.04159592529711375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [20:46<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.02662320021733225\n",
      "EPOCH 2:, train_loss = 0.22220868770611416, valid_loss = 0.23027723101345268 f1_macro = 0.02662320021733225 acc = 0.04159592529711375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [20:46<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.02662320021733225\n",
      "EPOCH 3:, train_loss = 0.22047424956564438, valid_loss = 0.23013676300242142 f1_macro = 0.02662320021733225 acc = 0.04159592529711375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [20:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.02662320021733225\n",
      "EPOCH 4:, train_loss = 0.2204507927709538, valid_loss = 0.23062178895280167 f1_macro = 0.02662320021733225 acc = 0.04159592529711375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [20:46<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.02662320021733225\n",
      "EPOCH 5:, train_loss = 0.22044180867976088, valid_loss = 0.23114905848696427 f1_macro = 0.02662320021733225 acc = 0.04159592529711375\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, optimizer, criterion, train_loader, valid_loader, CFG.epochs, scheduler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      test_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for step, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "          y_preds = model(input_id, mask)\n",
    "          loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        test_loss.append(loss.detach().cpu().item())\n",
    "    test_loss = np.mean(test_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "\n",
    "    print(f'Test metrics: test_loss={test_loss}', *[f'{name} = {value}' for name, value in metrics.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(CFG.model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(CFG.test_df)\n",
    "test_df = df.loc[test_df['ID']]\n",
    "test_ds = TrainDataset(CFG, test_df)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss=0.3199112701254922 f1_macro = 0.02662320021733225 acc = 0.04159592529711375\n"
     ]
    }
   ],
   "source": [
    "test(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss=0.23148165897625247 f1_macro = 0.2102861154993856 acc = 0.46076923076923076\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
