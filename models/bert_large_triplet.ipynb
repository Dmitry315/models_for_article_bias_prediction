{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  dataset='./dataset/polarization_ds.csv'\n",
    "  train_df='./dataset/media_split/train.csv'\n",
    "  valid_df='./dataset/media_split/valid.csv'\n",
    "  test_df='./dataset/media_split/test.csv'\n",
    "  target_cols=['left', 'center', 'right'] # 'bias_text'\n",
    "  classes=3\n",
    "  model='bert-large-cased'\n",
    "  embedd_dim=1024\n",
    "  criterion = 'mse' # ['crossentropy', 'mse', 'l1', 'focal']\n",
    "  main_metric = 'f1_macro'\n",
    "  model_file = './models/bert_large_triplet.pt'\n",
    "  # just use it\n",
    "  apex=True\n",
    "  gradient_checkpointing=True\n",
    "  num_cycles=0.5\n",
    "  num_warmup_steps=0\n",
    "  epochs=5\n",
    "  encoder_lr=2e-5\n",
    "  decoder_lr=2e-5\n",
    "  min_lr=1e-6\n",
    "  eps=1e-6\n",
    "  betas=(0.9, 0.999)\n",
    "  batch_size=8\n",
    "  max_len=512\n",
    "  weight_decay=0.01\n",
    "  # gradient_accumulation_steps=1\n",
    "  max_grad_norm=1\n",
    "  seed=0\n",
    "  scheduler='cosine' # ['linear', 'cosine']\n",
    "  batch_scheduler=True\n",
    "  #\n",
    "  colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import os\n",
    "  os.chdir('/content/drive/MyDrive/lab/bert_finetune')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available(): # для GPU отдельный seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(CFG.seed)\n",
    "# есть стохастические операции на GPU\n",
    "# сделаем их детерминированными для воспроизводимости\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG.dataset, index_col='id')\n",
    "train_df = pd.read_csv(CFG.train_df)\n",
    "valid_df = pd.read_csv(CFG.valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>source</th>\n",
       "      <th>bias</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>authors</th>\n",
       "      <th>content</th>\n",
       "      <th>content_original</th>\n",
       "      <th>source_url</th>\n",
       "      <th>bias_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wzYhj24VaSbhhr8T</th>\n",
       "      <td>sexual_misconduct</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://nypost.com/2020/05/03/two-more-people-...</td>\n",
       "      <td>Two more people back parts of Tara Reade’s cla...</td>\n",
       "      <td>2020-05-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two more people corroborated some of Tara Read...</td>\n",
       "      <td>Two more people corroborated some of Tara Read...</td>\n",
       "      <td>www.nypost.com</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcgmZXUEXb0xWWB1</th>\n",
       "      <td>marijuana_legalization</td>\n",
       "      <td>Washington Times</td>\n",
       "      <td>2.0</td>\n",
       "      <td>http://www.washingtontimes.com/news/2014/apr/2...</td>\n",
       "      <td>Colorado lawmakers set limits on pot edibles p...</td>\n",
       "      <td>2014-04-21</td>\n",
       "      <td>Valerie Richardson</td>\n",
       "      <td>DENVER — The Mile High City was jammed with po...</td>\n",
       "      <td>DENVER — The Mile High City was jammed with po...</td>\n",
       "      <td>www.washingtontimes.com</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oYxUHzLvAtQhGoXk</th>\n",
       "      <td>foreign_policy</td>\n",
       "      <td>TheBlaze.com</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.theblaze.com/news/2018/11/11/trump...</td>\n",
       "      <td>Trump, world leaders, mark 100-year WWI annive...</td>\n",
       "      <td>2018-11-11</td>\n",
       "      <td>Teri Webster</td>\n",
       "      <td>President Donald Trump and dozens of world lea...</td>\n",
       "      <td>President Donald Trump and dozens of world lea...</td>\n",
       "      <td>www.theblaze.com</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   topic            source  bias   \n",
       "id                                                                 \n",
       "wzYhj24VaSbhhr8T       sexual_misconduct     New York Post   2.0  \\\n",
       "pcgmZXUEXb0xWWB1  marijuana_legalization  Washington Times   2.0   \n",
       "oYxUHzLvAtQhGoXk          foreign_policy      TheBlaze.com   2.0   \n",
       "\n",
       "                                                                url   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  https://nypost.com/2020/05/03/two-more-people-...  \\\n",
       "pcgmZXUEXb0xWWB1  http://www.washingtontimes.com/news/2014/apr/2...   \n",
       "oYxUHzLvAtQhGoXk  https://www.theblaze.com/news/2018/11/11/trump...   \n",
       "\n",
       "                                                              title   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  Two more people back parts of Tara Reade’s cla...  \\\n",
       "pcgmZXUEXb0xWWB1  Colorado lawmakers set limits on pot edibles p...   \n",
       "oYxUHzLvAtQhGoXk  Trump, world leaders, mark 100-year WWI annive...   \n",
       "\n",
       "                        date             authors   \n",
       "id                                                 \n",
       "wzYhj24VaSbhhr8T  2020-05-03                 NaN  \\\n",
       "pcgmZXUEXb0xWWB1  2014-04-21  Valerie Richardson   \n",
       "oYxUHzLvAtQhGoXk  2018-11-11        Teri Webster   \n",
       "\n",
       "                                                            content   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  Two more people corroborated some of Tara Read...  \\\n",
       "pcgmZXUEXb0xWWB1  DENVER — The Mile High City was jammed with po...   \n",
       "oYxUHzLvAtQhGoXk  President Donald Trump and dozens of world lea...   \n",
       "\n",
       "                                                   content_original   \n",
       "id                                                                    \n",
       "wzYhj24VaSbhhr8T  Two more people corroborated some of Tara Read...  \\\n",
       "pcgmZXUEXb0xWWB1  DENVER — The Mile High City was jammed with po...   \n",
       "oYxUHzLvAtQhGoXk  President Donald Trump and dozens of world lea...   \n",
       "\n",
       "                               source_url bias_text  \n",
       "id                                                   \n",
       "wzYhj24VaSbhhr8T           www.nypost.com     right  \n",
       "pcgmZXUEXb0xWWB1  www.washingtontimes.com     right  \n",
       "oYxUHzLvAtQhGoXk         www.theblaze.com     right  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = (df['bias_text'] == 'center') + (df['bias_text'] == 'right') * 2\n",
    "df['bias_text'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['left'] = (df['bias_text'] == 0).astype(float)\n",
    "df['center'] = (df['bias_text'] == 1).astype(float)\n",
    "df['right'] = (df['bias_text'] == 2).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[train_df['ID']]\n",
    "valid_df = df.loc[valid_df['ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>source</th>\n",
       "      <th>bias</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>authors</th>\n",
       "      <th>content</th>\n",
       "      <th>content_original</th>\n",
       "      <th>source_url</th>\n",
       "      <th>bias_text</th>\n",
       "      <th>left</th>\n",
       "      <th>center</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zl7kc7EmAyIdUMIo</th>\n",
       "      <td>immigration</td>\n",
       "      <td>National Review</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.nationalreview.com/2018/12/governm...</td>\n",
       "      <td>Shutdown Theater, Again</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>Kevin D. Williamson, Kyle Smith, Andrew C. Mcc...</td>\n",
       "      <td>President Trump and Senate Minority Leader Chu...</td>\n",
       "      <td>President Trump and Senate Minority Leader Chu...</td>\n",
       "      <td>www.nationalreview.com</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xpbjYTJYPdlw6HmJ</th>\n",
       "      <td>culture</td>\n",
       "      <td>Yahoo! The 360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://news.yahoo.com/can-the-developing-worl...</td>\n",
       "      <td>Can the developing world endure the coronavirus?</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>Mike Bebernes</td>\n",
       "      <td>“ The 360 ” shows you diverse perspectives on ...</td>\n",
       "      <td>“The 360” shows you diverse perspectives on th...</td>\n",
       "      <td>www.news.yahoo.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k4SGI3GXarnz5dJl</th>\n",
       "      <td>elections</td>\n",
       "      <td>Politico</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://www.politico.com/story/2016/07/bernie-s...</td>\n",
       "      <td>Sanders’ California supporters can’t quite say...</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>Daniel Strauss, Henry C. Jackson, Nick Gass</td>\n",
       "      <td>LOS ANGELES — Actress Rosario Dawson took the ...</td>\n",
       "      <td>LOS ANGELES — Actress Rosario Dawson took the ...</td>\n",
       "      <td>www.politico.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0jIpietfnrPRGHKQ</th>\n",
       "      <td>white_house</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.businessinsider.com/trump-distance...</td>\n",
       "      <td>Trump says he doesn't know if Rudy Giuliani is...</td>\n",
       "      <td>2019-10-11</td>\n",
       "      <td>Sonam Sheth</td>\n",
       "      <td>President Donald Trump said on Friday that he ...</td>\n",
       "      <td>President Donald Trump said on Friday that he ...</td>\n",
       "      <td>www.businessinsider.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zMlSt7dyJvanHqJq</th>\n",
       "      <td>politics</td>\n",
       "      <td>CNN (Web News)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://www.cnn.com/2017/01/20/politics/donald-...</td>\n",
       "      <td>Trump's historic moment arrives</td>\n",
       "      <td>2017-01-20</td>\n",
       "      <td>Stephen Collinson</td>\n",
       "      <td>Washington ( CNN ) Donald Trump became the 45t...</td>\n",
       "      <td>Washington (CNN) Donald Trump became the 45th ...</td>\n",
       "      <td>www.cnn.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        topic            source  bias   \n",
       "id                                                      \n",
       "zl7kc7EmAyIdUMIo  immigration   National Review   2.0  \\\n",
       "xpbjYTJYPdlw6HmJ      culture    Yahoo! The 360   1.0   \n",
       "k4SGI3GXarnz5dJl    elections          Politico   0.0   \n",
       "0jIpietfnrPRGHKQ  white_house  Business Insider   1.0   \n",
       "zMlSt7dyJvanHqJq     politics    CNN (Web News)   0.0   \n",
       "\n",
       "                                                                url   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo  https://www.nationalreview.com/2018/12/governm...  \\\n",
       "xpbjYTJYPdlw6HmJ  https://news.yahoo.com/can-the-developing-worl...   \n",
       "k4SGI3GXarnz5dJl  http://www.politico.com/story/2016/07/bernie-s...   \n",
       "0jIpietfnrPRGHKQ  https://www.businessinsider.com/trump-distance...   \n",
       "zMlSt7dyJvanHqJq  http://www.cnn.com/2017/01/20/politics/donald-...   \n",
       "\n",
       "                                                              title   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo                            Shutdown Theater, Again  \\\n",
       "xpbjYTJYPdlw6HmJ   Can the developing world endure the coronavirus?   \n",
       "k4SGI3GXarnz5dJl  Sanders’ California supporters can’t quite say...   \n",
       "0jIpietfnrPRGHKQ  Trump says he doesn't know if Rudy Giuliani is...   \n",
       "zMlSt7dyJvanHqJq                    Trump's historic moment arrives   \n",
       "\n",
       "                        date   \n",
       "id                             \n",
       "zl7kc7EmAyIdUMIo  2018-12-12  \\\n",
       "xpbjYTJYPdlw6HmJ  2020-06-30   \n",
       "k4SGI3GXarnz5dJl  2016-07-02   \n",
       "0jIpietfnrPRGHKQ  2019-10-11   \n",
       "zMlSt7dyJvanHqJq  2017-01-20   \n",
       "\n",
       "                                                            authors   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo  Kevin D. Williamson, Kyle Smith, Andrew C. Mcc...  \\\n",
       "xpbjYTJYPdlw6HmJ                                      Mike Bebernes   \n",
       "k4SGI3GXarnz5dJl        Daniel Strauss, Henry C. Jackson, Nick Gass   \n",
       "0jIpietfnrPRGHKQ                                        Sonam Sheth   \n",
       "zMlSt7dyJvanHqJq                                  Stephen Collinson   \n",
       "\n",
       "                                                            content   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo  President Trump and Senate Minority Leader Chu...  \\\n",
       "xpbjYTJYPdlw6HmJ  “ The 360 ” shows you diverse perspectives on ...   \n",
       "k4SGI3GXarnz5dJl  LOS ANGELES — Actress Rosario Dawson took the ...   \n",
       "0jIpietfnrPRGHKQ  President Donald Trump said on Friday that he ...   \n",
       "zMlSt7dyJvanHqJq  Washington ( CNN ) Donald Trump became the 45t...   \n",
       "\n",
       "                                                   content_original   \n",
       "id                                                                    \n",
       "zl7kc7EmAyIdUMIo  President Trump and Senate Minority Leader Chu...  \\\n",
       "xpbjYTJYPdlw6HmJ  “The 360” shows you diverse perspectives on th...   \n",
       "k4SGI3GXarnz5dJl  LOS ANGELES — Actress Rosario Dawson took the ...   \n",
       "0jIpietfnrPRGHKQ  President Donald Trump said on Friday that he ...   \n",
       "zMlSt7dyJvanHqJq  Washington (CNN) Donald Trump became the 45th ...   \n",
       "\n",
       "                               source_url  bias_text  left  center  right  \n",
       "id                                                                         \n",
       "zl7kc7EmAyIdUMIo   www.nationalreview.com          2   0.0     0.0    1.0  \n",
       "xpbjYTJYPdlw6HmJ       www.news.yahoo.com          1   0.0     1.0    0.0  \n",
       "k4SGI3GXarnz5dJl         www.politico.com          0   1.0     0.0    0.0  \n",
       "0jIpietfnrPRGHKQ  www.businessinsider.com          1   0.0     1.0    0.0  \n",
       "zMlSt7dyJvanHqJq              www.cnn.com          0   1.0     0.0    0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = train_df[['bias', 'source_url']].groupby(by = 'source_url').agg(['min', 'max'])['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['www.aljazeera.com', 'www.blogs.wsj.com', 'www.bostonglobe.com',\n",
       "       'www.cbsnews.com', 'www.chicago.suntimes.com', 'www.cnn.com',\n",
       "       'www.csmonitor.com', 'www.dailymail.co.uk', 'www.foxnews.com',\n",
       "       'www.marketwatch.com', 'www.nationalreview.com', 'www.news.yahoo.com',\n",
       "       'www.npr.org', 'www.nypost.com', 'www.politico.com',\n",
       "       'www.scientificamerican.com', 'www.theatlantic.com',\n",
       "       'www.thedailybeast.com', 'www.thehill.com', 'www.theweek.com',\n",
       "       'www.time.com', 'www.townhall.com', 'www.usatoday.com', 'www.vox.com',\n",
       "       'www.washingtontimes.com', 'www.wsj.com'],\n",
       "      dtype='object', name='source_url')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources_for_triplet = biases[biases['min'] != biases['max']].index\n",
    "sources_for_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26590, 14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24294, 14)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_df = train_df[train_df['source_url'].isin(sources_for_triplet)]\n",
    "triplet_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = (df['title'] + ' ' + df['content']).values\n",
    "        self.labels = df['bias_text'].values\n",
    "        self.media = df['source_url'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_anchor(self, idx):\n",
    "        # tokenization\n",
    "        inputs = self.cfg.tokenizer.encode_plus(\n",
    "            self.texts[idx], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return inputs, labels\n",
    "\n",
    "    def get_pos(self, idx):\n",
    "        media = self.media[idx]\n",
    "        labels = self.labels[idx]\n",
    "        idxs = np.arange(len(self.texts))[(self.labels == labels) & (self.media != media)]\n",
    "        new_idx = np.random.choice(idxs)\n",
    "        return self.get_anchor(new_idx)\n",
    "    \n",
    "    def get_neg(self, idx):\n",
    "        media = self.media[idx]\n",
    "        labels = self.labels[idx]\n",
    "        idxs = np.arange(len(self.texts))[(self.labels != labels) & (self.media == media)]\n",
    "        new_idx = np.random.choice(idxs)\n",
    "        return self.get_anchor(new_idx)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        inputs_a, label_a = self.get_anchor(idx)\n",
    "        inputs_p, label_p = self.get_pos(idx)\n",
    "        inputs_n, label_n = self.get_neg(idx)\n",
    "        return inputs_a, inputs_p, inputs_n\n",
    "\n",
    "def collate(inputs):\n",
    "\t\t# reduce sequence length\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.model)\n",
    "        if CFG.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.clf =  nn.Linear(CFG.embedd_dim, CFG.classes)\n",
    "        if not CFG.criterion == 'crossentropy':\n",
    "            self.sm = nn.Softmax(dim=-1)\n",
    "        torch.nn.init.xavier_uniform_(self.clf.weight)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        # sequence has [CLF] token in the beginning\n",
    "        # bert() returns first vector as pooling of sentence\n",
    "        _, x = self.model(input_ids= input_id, attention_mask=mask, return_dict=False)\n",
    "        # out = self.clf(x)\n",
    "        # if not CFG.criterion == 'crossentropy':\n",
    "        #     return self.sm(out)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop, metrics and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, pred):\n",
    "  y = torch.argmax(y, dim=1)\n",
    "  pred = torch.argmax(pred, dim=1)\n",
    "  f1 = f1_score(y, pred, average='macro')\n",
    "  acc = accuracy_score(y, pred)\n",
    "  return {'f1_macro': f1, 'acc': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.reduction == 'mean': \n",
    "          return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "          return loss.sum()\n",
    "        else:\n",
    "          raise NotImplementedError(f'Not implemented reduction: {self.reduction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, pred_a, pred_p, pred_n):\n",
    "        ap = torch.sqrt(self.mse(pred_a, pred_p).sum(dim=-1))\n",
    "        an = torch.sqrt(self.mse(pred_a, pred_n).sum(dim=-1))\n",
    "        return self.relu(ap - an + self.eps).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_ds = TripletDataset(CFG, triplet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trip_ds, batch_size=CFG.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = TripletLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_parameters = get_optimizer_params(model,\n",
    "#                                            encoder_lr=CFG.encoder_lr, \n",
    "#                                            decoder_lr=CFG.decoder_lr,\n",
    "#                                            weight_decay=CFG.weight_decay)\n",
    "#optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=CFG.encoder_lr, weight_decay=1e-4, eps=CFG.eps, betas=CFG.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)\n",
    "scheduler = get_scheduler(CFG, optimizer, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TripletLoss(\n",
       "  (mse): MSELoss()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(CFG.device)\n",
    "criterion.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, train_loader, epochs, scheduler):\n",
    "  best_score = 0\n",
    "  # multiplies gradient so it won't vanish (torch use float16)\n",
    "  scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "  for e in range(epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for inputs_a, inputs_p, inputs_n in tqdm(train_loader):\n",
    "      inputs_a = collate(inputs_a)\n",
    "      inputs_p = collate(inputs_p)\n",
    "      inputs_n = collate(inputs_n)\n",
    "      # move inputs to device\n",
    "      mask_a = inputs_a['attention_mask'].to(CFG.device)\n",
    "      input_id_a = inputs_a['input_ids'].squeeze(1).to(CFG.device)\n",
    "      mask_p = inputs_p['attention_mask'].to(CFG.device)\n",
    "      input_id_p = inputs_p['input_ids'].squeeze(1).to(CFG.device)\n",
    "      mask_n = inputs_n['attention_mask'].to(CFG.device)\n",
    "      input_id_n = inputs_n['input_ids'].squeeze(1).to(CFG.device)\n",
    "      # forward\n",
    "      with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "          pred_a = model(input_id_a, mask_a)\n",
    "          pred_p = model(input_id_p, mask_p)\n",
    "          pred_n = model(input_id_n, mask_n)\n",
    "          loss = criterion(pred_a, pred_p, pred_n)\n",
    "      # calculate loss\n",
    "      train_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      scaler.scale(loss).backward()\n",
    "      # gradient clipping\n",
    "      grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "      if CFG.batch_scheduler:\n",
    "          scheduler.step()\n",
    "    train_loss = np.mean(train_loss)\n",
    "    print(f'EPOCH {e + 1}:, train_loss = {train_loss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3037/3037 [1:01:56<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:, train_loss = 0.013815938194993918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3037/3037 [1:01:53<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2:, train_loss = 7.812964524129023e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3037/3037 [1:01:52<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3:, train_loss = 4.8298887953777385e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3037/3037 [1:01:51<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4:, train_loss = 4.744101919715035e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3037/3037 [1:01:51<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5:, train_loss = 4.417610556736843e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, optimizer, criterion, train_loader, CFG.epochs, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), CFG.model_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      test_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for step, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        # with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "        y_preds = model(input_id, mask)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        test_loss.append(loss.detach().cpu().item())\n",
    "    test_loss = np.mean(test_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "\n",
    "    print(f'Test metrics: test_loss={test_loss}', *[f'{name} = {value}' for name, value in metrics.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(CFG.model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(CFG.test_df)\n",
    "test_df = df.loc[test_df['ID']]\n",
    "test_ds = TrainDataset(CFG, test_df)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss=0.4329216697731534 f1_macro = 0.2260958910521593 acc = 0.2593378607809847\n"
     ]
    }
   ],
   "source": [
    "test(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss=0.2522725470182372 f1_macro = 0.4679760415932736 acc = 0.5246153846153846\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
